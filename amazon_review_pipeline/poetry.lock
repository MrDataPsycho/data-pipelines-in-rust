[[package]]
name = "py4j"
version = "0.10.9.5"
description = "Enables Python programs to dynamically access arbitrary Java objects"
category = "main"
optional = false
python-versions = "*"

[[package]]
name = "pyspark"
version = "3.3.0"
description = "Apache Spark Python API"
category = "main"
optional = false
python-versions = ">=3.7"

[package.dependencies]
py4j = "0.10.9.5"

[package.extras]
ml = ["numpy (>=1.15)"]
mllib = ["numpy (>=1.15)"]
pandas_on_spark = ["numpy (>=1.15)", "pandas (>=1.0.5)", "pyarrow (>=1.0.0)"]
sql = ["pandas (>=1.0.5)", "pyarrow (>=1.0.0)"]

[metadata]
lock-version = "1.1"
python-versions = "^3.8"
content-hash = "797f045af0149bf58bc0ba3b28e3757df6ac2964ea6cc9dfd83ac1b96ed88397"

[metadata.files]
py4j = [
    {file = "py4j-0.10.9.5-py2.py3-none-any.whl", hash = "sha256:52d171a6a2b031d8a5d1de6efe451cf4f5baff1a2819aabc3741c8406539ba04"},
    {file = "py4j-0.10.9.5.tar.gz", hash = "sha256:276a4a3c5a2154df1860ef3303a927460e02e97b047dc0a47c1c3fb8cce34db6"},
]
pyspark = [
    {file = "pyspark-3.3.0.tar.gz", hash = "sha256:7ebe8e9505647b4d124d5a82fca60dfd3891021cf8ad6c5ec88777eeece92cf7"},
]
